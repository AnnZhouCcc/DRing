{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate choicefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficfilenamearr = [\"unv1\",\"prv1\",\"cluster_a\",\"cluster_b\",\"cluster_c\"]\n",
    "numintervalarr = [4,4,24,24,24]\n",
    "for itrafficfilename,trafficfilename in enumerate(trafficfilenamearr):\n",
    "    numinterval = numintervalarr[itrafficfilename]\n",
    "    choicefile = f\"/home/annzhou/DRing/src/emp/datacentre/choicefiles/{trafficfilename}\"\n",
    "    with open(choicefile,'w') as f:\n",
    "        for _ in range(numinterval):\n",
    "            f.write(\"5\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate qivarfiles for meta\n",
    "inspired by computerouting2/computerouting2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy(copyfromfile, copyintofile):\n",
    "    with open(copyfromfile,'r') as fr:\n",
    "        lines = fr.readlines()\n",
    "        with open(copyintofile,'w') as fw:\n",
    "            for line in lines:\n",
    "                tokens = line.split()\n",
    "                fromsw = int(tokens[0])\n",
    "                tosw = int(tokens[1])\n",
    "                pid = int(tokens[2])\n",
    "                value = float(tokens[5])\n",
    "                fw.write(f\"{fromsw},{tosw},{pid},{value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterarr = [\"a\",\"b\",\"c\"]\n",
    "intervalstart = 24\n",
    "intervalend = 48\n",
    "factor = 64\n",
    "numsw = 80\n",
    "racke = 0\n",
    "step = 1800\n",
    "\n",
    "for cluster in clusterarr:\n",
    "    for interval in range(intervalstart, intervalend):\n",
    "        copyfromivarfile = f\"/home/annzhou/DRing/src/emp/datacentre/pathweightfiles/dring/racke{racke}/cluster{cluster}/pathweight_dring_racke{racke}_cluster{cluster}_lp1_barriernocrossover_{interval*step}_{(interval+1)*step}_{factor}.txt\"\n",
    "        copyintoqivarfile = f\"/home/annzhou/DRing/src/emp/datacentre/qivarfiles/qivar_dring_0_0_cluster_{cluster}_{interval}_racke{racke}_2_0\"\n",
    "        copy(copyfromivarfile, copyintoqivarfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate qivarfiles for wisc\n",
    "- compute: inspired by computerouting2/testtemporalH_wisc.py\n",
    "- quantize: inspired by makepathweightfiles/makepathweightfiles.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "trafficname = \"prv1\"\n",
    "scaledown = 10000000\n",
    "H = 1\n",
    "homedir = \"/home/annzhou\"\n",
    "routingname = \"racke0\"\n",
    "\n",
    "\n",
    "def compute(interval,graphfile,numsw,netpathfile,traffic,method,crossover,qivarfile):\n",
    "    # read netpathfile\n",
    "    netpath = list()\n",
    "    for i in range(numsw):\n",
    "        netpath.append(list())\n",
    "        for j in range(numsw):\n",
    "            netpath[i].append(list())\n",
    "    with open(netpathfile,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        # produce\n",
    "        fromsw = 0\n",
    "        tosw = 0\n",
    "        for line in lines:\n",
    "            if \"->\" not in line:\n",
    "                tokens = line.split()\n",
    "                fromsw = int(tokens[0])\n",
    "                tosw = int(tokens[1])\n",
    "            else:\n",
    "                path = [fromsw]\n",
    "                tokens = line.split()\n",
    "                for token in tokens:\n",
    "                    hops = token.split(\"->\")\n",
    "                    path.append(int(hops[1]))\n",
    "                netpath[fromsw][tosw].append(path)\n",
    "\n",
    "        # check\n",
    "        for line in lines:\n",
    "            if \"->\" not in line:\n",
    "                tokens = line.split()\n",
    "                fromsw = int(tokens[0])\n",
    "                tosw = int(tokens[1])\n",
    "                numpaths = int(tokens[2])\n",
    "                if len(netpath[fromsw][tosw])!=numpaths:\n",
    "                    print(f\"ERROR: netpath is wrong, fromsw={fromsw}, tosw={tosw}, numpaths from file={numpaths}, numpaths from array={len(netpath[fromsw][tosw])}\")\n",
    "\n",
    "    # read graphfile\n",
    "    link = list()\n",
    "    for i in range(numsw):\n",
    "        link.append(list())\n",
    "        for j in range(numsw):\n",
    "            link[i].append(0)\n",
    "    with open(graphfile,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.split(\"->\")\n",
    "            fromsw = int(tokens[0])\n",
    "            tosw = int(tokens[1])\n",
    "            link[fromsw][tosw] = 1\n",
    "            link[tosw][fromsw] = 1\n",
    "\n",
    "    # read linkfailurefile (if needed)\n",
    "    # if numfaillink > 0:\n",
    "    #     with open(linkfailurefile,'r') as f:\n",
    "    #         lines = f.readlines()\n",
    "    #         for line in lines:\n",
    "    #             tokens = line.split()\n",
    "    #             fromsw = int(tokens[0])\n",
    "    #             tosw = int(tokens[1])\n",
    "    #             if link[fromsw][tosw] != 1:\n",
    "    #                 print(f\"ERROR: should have a link from {fromsw} to {tosw} but not\")\n",
    "    #             else:\n",
    "    #                 link[fromsw][tosw] /= 2\n",
    "\n",
    "    # read serverfile\n",
    "    # serverdict = dict()\n",
    "    # with open(serverfile,'r') as f:\n",
    "    #     lines = f.readlines()\n",
    "    #     for line in lines:\n",
    "    #         tokens = line.split(',')\n",
    "    #         serverdict[int(tokens[0])] = int(tokens[1])\n",
    "\n",
    "    # read flowfile\n",
    "    # traffic = list()\n",
    "    # for i in range(numsw):\n",
    "    #     traffic.append(list())\n",
    "    #     for j in range(numsw):\n",
    "    #         traffic[i].append(0)\n",
    "    # with open(flowfile,'r') as f:\n",
    "    #     lines = f.readlines()\n",
    "    #     for line in lines:\n",
    "    #         tokens = line.split(\",\")\n",
    "    #         fromsvr = int(tokens[0])\n",
    "    #         tosvr = int(tokens[1])\n",
    "    #         if fromsvr>=numserver or tosvr>=numserver: continue\n",
    "    #         fromsw = serverdict[fromsvr]\n",
    "    #         tosw = serverdict[tosvr]\n",
    "    #         if fromsw == tosw: continue\n",
    "    #         flowbytes = int(tokens[2])\n",
    "    #         traffic[fromsw][tosw] += flowbytes\n",
    "\n",
    "\n",
    "    # precompute\n",
    "    flowsvialink = list()\n",
    "    for i in range(numsw):\n",
    "        flowsvialink.append(list())\n",
    "        for j in range(numsw):\n",
    "            flowsvialink[i].append(list())\n",
    "    for fromsw in range(numsw):\n",
    "        for tosw in range(numsw):\n",
    "            if traffic[interval][fromsw][tosw] > 0:\n",
    "                for pid,path in enumerate(netpath[fromsw][tosw]):\n",
    "                    fidpidstr = f\"{fromsw},{tosw},{pid}\"\n",
    "                    prevhop = fromsw\n",
    "                    for hop in path[1:]:\n",
    "                        flowsvialink[prevhop][hop].append(fidpidstr)\n",
    "                        prevhop = hop\n",
    "\n",
    "    # for fromsw in range(numsw):\n",
    "    #     for tosw in range(numsw):\n",
    "    #         traffic[interval][fromsw][tosw] /= 100000\n",
    "\n",
    "\n",
    "    # Create a new model\n",
    "    model = gp.Model(\"mcf\")\n",
    "\n",
    "    # Add variables\n",
    "    maxpid = 0\n",
    "    for fromsw in range(numsw):\n",
    "        for tosw in range(numsw):\n",
    "            maxpid = max(maxpid,len(netpath[fromsw][tosw]))\n",
    "    vararr = list()\n",
    "    for fromsw in range(numsw):\n",
    "        vararr.append(list())\n",
    "        for tosw in range(numsw):\n",
    "            vararr[fromsw].append(list())\n",
    "            for pid in range(maxpid):\n",
    "                vararr[fromsw][tosw].append(None)\n",
    "    for fromsw in range(numsw):\n",
    "        for tosw in range(numsw):\n",
    "            if traffic[interval][fromsw][tosw] > 0:\n",
    "                for pid in range(len(netpath[fromsw][tosw])):\n",
    "                    var = model.addVar(name=f\"{fromsw}_{tosw}_{pid}\")\n",
    "                    vararr[fromsw][tosw][pid] = var\n",
    "    k = model.addVar(name=\"k\")\n",
    "\n",
    "    # Set objective function\n",
    "    model.setObjective(k, GRB.MAXIMIZE)\n",
    "\n",
    "    # Add constraints\n",
    "    # Constraint 0: for each fid: sum(pid) >= k * traffic[fid.from][fid.to]\n",
    "    for fromsw in range(numsw):\n",
    "        for tosw in range(numsw):\n",
    "            if traffic[interval][fromsw][tosw] > 0:\n",
    "                varlist = list()\n",
    "                for pid in range(len(netpath[fromsw][tosw])):\n",
    "                    varlist.append(vararr[fromsw][tosw][pid])\n",
    "                model.addConstr(sum(varlist)>=k*traffic[interval][fromsw][tosw],f\"c0_{fromsw}_{tosw}\")\n",
    "\n",
    "    # Constraint 1: for each link: sum(fid_pid) <= link[link.from][link.to]\n",
    "    for linkfrom in range(numsw):\n",
    "        for linkto in range(numsw):\n",
    "            flowstrlist = flowsvialink[linkfrom][linkto]\n",
    "            if len(flowstrlist) > 0:\n",
    "                varlist = list()\n",
    "                for flowstr in flowstrlist:\n",
    "                    tokens = flowstr.split(',')\n",
    "                    flowfrom = int(tokens[0])\n",
    "                    flowto = int(tokens[1])\n",
    "                    pid = int(tokens[2])\n",
    "                    varlist.append(vararr[flowfrom][flowto][pid])\n",
    "                model.addConstr(sum(varlist)<=link[linkfrom][linkto],f\"c1_{linkfrom}_{linkto}\")\n",
    "\n",
    "    # Constraint 2: for each fid: for each pid: pid = prev_pid\n",
    "    # for fromsw in range(numsw):\n",
    "    #     for tosw in range(numsw):\n",
    "    #         if traffic[interval][fromsw][tosw] > 0:\n",
    "    #             for pid in range(1,len(netpath[fromsw][tosw])):\n",
    "    #                 model.addConstr(vararr[fromsw][tosw][pid]-vararr[fromsw][tosw][pid-1]==0,f\"c2_{fromsw}_{tosw}_{pid}\")\n",
    "\n",
    "    # Optimize model\n",
    "    model.setParam('Method',method)\n",
    "    model.setParam('Crossover',crossover)\n",
    "    model.optimize()\n",
    "    # model.write(modelfile)\n",
    "\n",
    "    # Print results\n",
    "    if model.status == GRB.OPTIMAL:\n",
    "        print(f\"Optimal objective value: {model.objVal}\")\n",
    "        with open(qivarfile,'a') as f:\n",
    "            f.write(f\"{routingname},{interval},{k.x}\\n\")\n",
    "            for fromsw in range(numsw):\n",
    "                for tosw in range(numsw):\n",
    "                    if traffic[interval][fromsw][tosw] > 0:\n",
    "                        for pid in range(len(netpath[fromsw][tosw])):\n",
    "                            var = vararr[fromsw][tosw][pid]\n",
    "                            f.write(f\"{var.varName},{var.x}\\n\")\n",
    "    else:\n",
    "        print(\"No optimal solution found\")\n",
    "    return\n",
    "\n",
    "\n",
    "def solve(computeinterval,computeqivarfile,solveinterval,graphfile,numsw,netpathfile,traffic,cachefile,method,crossover):\n",
    "    if os.path.exists(cachefile):\n",
    "        with open(cachefile,'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                tokens = line.split(',')\n",
    "                mycomputeinterval = int(tokens[0])\n",
    "                mysolveinterval = int(tokens[1])\n",
    "                mythroughput = float(tokens[2])\n",
    "                if computeinterval==mycomputeinterval and solveinterval==mysolveinterval:\n",
    "                    return mythroughput\n",
    "\n",
    "    if not os.path.exists(computeqivarfile):\n",
    "        compute(computeinterval,graphfile,numsw,netpathfile,traffic,method,crossover,computeqivarfile)\n",
    "\n",
    "\n",
    "    # read netpathfile\n",
    "    netpath = list()\n",
    "    for i in range(numsw):\n",
    "        netpath.append(list())\n",
    "        for j in range(numsw):\n",
    "            netpath[i].append(list())\n",
    "    with open(netpathfile,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        # produce\n",
    "        fromsw = 0\n",
    "        tosw = 0\n",
    "        for line in lines:\n",
    "            if \"->\" not in line:\n",
    "                tokens = line.split()\n",
    "                fromsw = int(tokens[0])\n",
    "                tosw = int(tokens[1])\n",
    "            else:\n",
    "                path = [fromsw]\n",
    "                tokens = line.split()\n",
    "                for token in tokens:\n",
    "                    hops = token.split(\"->\")\n",
    "                    path.append(int(hops[1]))\n",
    "                netpath[fromsw][tosw].append(path)\n",
    "\n",
    "        # check\n",
    "        for line in lines:\n",
    "            if \"->\" not in line:\n",
    "                tokens = line.split()\n",
    "                fromsw = int(tokens[0])\n",
    "                tosw = int(tokens[1])\n",
    "                numpaths = int(tokens[2])\n",
    "                if len(netpath[fromsw][tosw])!=numpaths:\n",
    "                    print(f\"ERROR: netpath is wrong, fromsw={fromsw}, tosw={tosw}, numpaths from file={numpaths}, numpaths from array={len(netpath[fromsw][tosw])}\")\n",
    "\n",
    "    # read graphfile\n",
    "    link = list()\n",
    "    for i in range(numsw):\n",
    "        link.append(list())\n",
    "        for j in range(numsw):\n",
    "            link[i].append(0)\n",
    "    with open(graphfile,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.split(\"->\")\n",
    "            fromsw = int(tokens[0])\n",
    "            tosw = int(tokens[1])\n",
    "            link[fromsw][tosw] = 1\n",
    "            link[tosw][fromsw] = 1\n",
    "\n",
    "    # read linkfailurefile (if needed)\n",
    "    # if numfaillink > 0:\n",
    "    #     with open(linkfailurefile,'r') as f:\n",
    "    #         lines = f.readlines()\n",
    "    #         for line in lines:\n",
    "    #             tokens = line.split()\n",
    "    #             fromsw = int(tokens[0])\n",
    "    #             tosw = int(tokens[1])\n",
    "    #             if link[fromsw][tosw] != 1:\n",
    "    #                 print(f\"ERROR: should have a link from {fromsw} to {tosw} but not\")\n",
    "    #             else:\n",
    "    #                 link[fromsw][tosw] /= 2\n",
    "\n",
    "    # read serverfile\n",
    "    # serverdict = dict()\n",
    "    # with open(serverfile,'r') as f:\n",
    "    #     lines = f.readlines()\n",
    "    #     for line in lines:\n",
    "    #         tokens = line.split(',')\n",
    "    #         serverdict[int(tokens[0])] = int(tokens[1])\n",
    "\n",
    "    # read flowfile\n",
    "    # traffic = list()\n",
    "    # for i in range(numsw):\n",
    "    #     traffic.append(list())\n",
    "    #     for j in range(numsw):\n",
    "    #         traffic[i].append(0)\n",
    "    # with open(flowfile,'r') as f:\n",
    "    #     lines = f.readlines()\n",
    "    #     for line in lines:\n",
    "    #         tokens = line.split(\",\")\n",
    "    #         fromsvr = int(tokens[0])\n",
    "    #         tosvr = int(tokens[1])\n",
    "    #         if fromsvr>=numserver or tosvr>=numserver: continue\n",
    "    #         fromsw = serverdict[fromsvr]\n",
    "    #         tosw = serverdict[tosvr]\n",
    "    #         if fromsw == tosw: continue\n",
    "    #         flowbytes = int(tokens[2])\n",
    "    #         traffic[fromsw][tosw] += flowbytes\n",
    "\n",
    "    # read computeqivarfile\n",
    "    pathweight = list()\n",
    "    for i in range(numsw):\n",
    "        pathweight.append(list())\n",
    "        for j in range(numsw):\n",
    "            pathweight[i].append(dict())\n",
    "    with open(computeqivarfile,'r') as f:\n",
    "        lines = f.readlines()[1:]\n",
    "        for line in lines:\n",
    "            tokens = line.split(\",\")\n",
    "            string = tokens[0]\n",
    "            stringtokens = string.split(\"_\")\n",
    "            fromsw = int(stringtokens[0])\n",
    "            tosw = int(stringtokens[1])\n",
    "            pid = int(stringtokens[2])\n",
    "            weight = float(tokens[1])\n",
    "            pathweight[fromsw][tosw][pid] = weight\n",
    "    for fromsw in range(numsw):\n",
    "        for tosw in range(numsw):\n",
    "            if traffic[solveinterval][fromsw][tosw]>0 and traffic[computeinterval][fromsw][tosw]==0:\n",
    "                numnetpath = len(netpath[fromsw][tosw])\n",
    "                for i in range(numnetpath):\n",
    "                    pathweight[fromsw][tosw][i] = 1.0/numnetpath\n",
    "\n",
    "\n",
    "    # precompute\n",
    "    flowsvialink = list()\n",
    "    for i in range(numsw):\n",
    "        flowsvialink.append(list())\n",
    "        for j in range(numsw):\n",
    "            flowsvialink[i].append(list())\n",
    "    for fromsw in range(numsw):\n",
    "        for tosw in range(numsw):\n",
    "            if traffic[solveinterval][fromsw][tosw] > 0:\n",
    "                for pid,path in enumerate(netpath[fromsw][tosw]):\n",
    "                    fidpidstr = f\"{fromsw},{tosw},{pid}\"\n",
    "                    prevhop = fromsw\n",
    "                    for hop in path[1:]:\n",
    "                        flowsvialink[prevhop][hop].append(fidpidstr)\n",
    "                        prevhop = hop\n",
    "\n",
    "    # for fromsw in range(numsw):\n",
    "    #     for tosw in range(numsw):\n",
    "    #         traffic[interval][fromsw][tosw] /= 100000\n",
    "\n",
    "\n",
    "    # Create a new model\n",
    "    model = gp.Model(\"mcf\")\n",
    "\n",
    "    # Add variables\n",
    "    maxpid = 0\n",
    "    for fromsw in range(numsw):\n",
    "        for tosw in range(numsw):\n",
    "            maxpid = max(maxpid,len(netpath[fromsw][tosw]))\n",
    "    vararr = list()\n",
    "    for fromsw in range(numsw):\n",
    "        vararr.append(list())\n",
    "        for tosw in range(numsw):\n",
    "            vararr[fromsw].append(list())\n",
    "            for pid in range(maxpid):\n",
    "                vararr[fromsw][tosw].append(None)\n",
    "    for fromsw in range(numsw):\n",
    "        for tosw in range(numsw):\n",
    "            if traffic[solveinterval][fromsw][tosw] > 0:\n",
    "                for pid in range(len(netpath[fromsw][tosw])):\n",
    "                    var = model.addVar(name=f\"{fromsw}_{tosw}_{pid}\")\n",
    "                    vararr[fromsw][tosw][pid] = var\n",
    "    k = model.addVar(name=\"k\")\n",
    "    avararr = list()\n",
    "    for fromsw in range(numsw):\n",
    "        avararr.append(list())\n",
    "        for tosw in range(numsw):\n",
    "            avararr[fromsw].append(None)\n",
    "    for fromsw in range(numsw):\n",
    "        for tosw in range(numsw):\n",
    "            if traffic[solveinterval][fromsw][tosw] > 0:\n",
    "                avar = model.addVar(name=f\"a_{fromsw}_{tosw}\")\n",
    "                avararr[fromsw][tosw] = avar\n",
    "\n",
    "    # Set objective function\n",
    "    model.setObjective(k, GRB.MAXIMIZE)\n",
    "\n",
    "    # Add constraints\n",
    "    # Constraint 0: for each fid: sum(pid) >= k * traffic[fid.from][fid.to]\n",
    "    for fromsw in range(numsw):\n",
    "        for tosw in range(numsw):\n",
    "            if traffic[solveinterval][fromsw][tosw] > 0:\n",
    "                varlist = list()\n",
    "                for pid in range(len(netpath[fromsw][tosw])):\n",
    "                    varlist.append(vararr[fromsw][tosw][pid])\n",
    "                model.addConstr(sum(varlist)>=k*traffic[solveinterval][fromsw][tosw],f\"c0_{fromsw}_{tosw}\")\n",
    "\n",
    "    # Constraint 1: for each link: sum(fid_pid) <= link[link.from][link.to]\n",
    "    for linkfrom in range(numsw):\n",
    "        for linkto in range(numsw):\n",
    "            flowstrlist = flowsvialink[linkfrom][linkto]\n",
    "            if len(flowstrlist) > 0:\n",
    "                varlist = list()\n",
    "                for flowstr in flowstrlist:\n",
    "                    tokens = flowstr.split(',')\n",
    "                    flowfrom = int(tokens[0])\n",
    "                    flowto = int(tokens[1])\n",
    "                    pid = int(tokens[2])\n",
    "                    varlist.append(vararr[flowfrom][flowto][pid])\n",
    "                model.addConstr(sum(varlist)<=link[linkfrom][linkto],f\"c1_{linkfrom}_{linkto}\")\n",
    "\n",
    "    # Constraint 2: for each fid: for each pid: pid = pathweight[fid][pid] * a[fid]\n",
    "    for fromsw in range(numsw):\n",
    "        for tosw in range(numsw):\n",
    "            if traffic[solveinterval][fromsw][tosw] > 0:\n",
    "                for pid in range(len(netpath[fromsw][tosw])):\n",
    "                    model.addConstr(vararr[fromsw][tosw][pid]-pathweight[fromsw][tosw][pid]*avararr[fromsw][tosw]==0,f\"c2_{fromsw}_{tosw}_{pid}\")\n",
    "\n",
    "    # Optimize model\n",
    "    model.setParam('Method',-1)\n",
    "    model.setParam('Crossover',0)\n",
    "    model.optimize()\n",
    "    # model.write(modelfile)\n",
    "\n",
    "    # Print results\n",
    "    if model.status == GRB.OPTIMAL:\n",
    "        print(f\"Optimal objective value: {model.objVal}\")\n",
    "        with open(cachefile,'a') as f:\n",
    "            f.write(f\"{computeinterval},{solveinterval},{k.x}\\n\")\n",
    "            # for fromsw in range(numsw):\n",
    "            #     for tosw in range(numsw):\n",
    "            #         if traffic[fromsw][tosw] > 0:\n",
    "            #             for pid in range(len(netpath[fromsw][tosw])):\n",
    "            #                 var = vararr[fromsw][tosw][pid]\n",
    "            #                 f.write(f\"{var.varName},{var.x}\\n\")\n",
    "    else:\n",
    "        print(\"No optimal solution found\")\n",
    "    return k.x\n",
    "\n",
    "\n",
    "# set up parameters\n",
    "# if trafficname == \"cluster_b\":\n",
    "#     trafficfilesuffixlist = [\"0_500\",\"500_1000\",\"1000_1500\",\"1500_2000\",\"2000_2500\",\"2500_2900\"]\n",
    "# else:\n",
    "#     trafficfilesuffixlist = [\"0_273\"]\n",
    "# step_s = 1800\n",
    "numserver = 2988\n",
    "numsw = 80\n",
    "numport = 64\n",
    "# numinterval = 86400//step_s\n",
    "numinterval = 8\n",
    "graphname = \"dring\"\n",
    "numfaillink = 0\n",
    "numlink = 2132\n",
    "fseed = 0\n",
    "method = 2\n",
    "crossover = 0\n",
    "factor = 64\n",
    "\n",
    "if graphname==\"dring\":\n",
    "    graphfile = f\"{homedir}/DRing/src/emp/datacentre/graphfiles/ring_supergraph/double_ring/instance1_{numsw}_{numport}.edgelist\"\n",
    "elif graphname==\"rrg\":\n",
    "    graphfile = f\"{homedir}/DRing/src/emp/datacentre/graphfiles/ring_supergraph/rrg/instance1_{numsw}_{numport}.edgelist\"\n",
    "else:\n",
    "    print(\"ERROR: leafspine does not need to compute path weight\")\n",
    "\n",
    "\n",
    "# read in traffic\n",
    "serverfile = f\"{homedir}/DRing/src/emp/datacentre/serverfiles/dring_{numserver}_{numsw}_{numport}\"\n",
    "serverdict = dict()\n",
    "with open(serverfile,'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        tokens = line.split(',')\n",
    "        serverdict[int(tokens[0])] = int(tokens[1])\n",
    "\n",
    "traffic = list()\n",
    "for k in range(numinterval):\n",
    "    traffic.append(list())\n",
    "    for i in range(numsw):\n",
    "        traffic[k].append(list())\n",
    "        for j in range(numsw):\n",
    "            traffic[k][i].append(0)\n",
    "# for suffix in trafficfilesuffixlist:\n",
    "#     trafficfile = f\"{homedir}/DRing/src/emp/datacentre/trafficfiles/{trafficname}/traffic_64racks_{suffix}\"\n",
    "#     with open(trafficfile,'r') as f:\n",
    "#         lines = f.readlines()\n",
    "#         for line in lines:\n",
    "#             tokens = line.split()\n",
    "#             time_s = int(tokens[0])\n",
    "#             interval = time_s//step_s\n",
    "#             fromsvr = int(tokens[1])\n",
    "#             tosvr = int(tokens[2])\n",
    "#             if fromsvr>=numserver or tosvr>=numserver: continue\n",
    "#             fromsw = serverdict[fromsvr]\n",
    "#             tosw = serverdict[tosvr]\n",
    "#             if fromsw == tosw: continue\n",
    "#             flowbytes = int(tokens[3])\n",
    "#             traffic[interval][fromsw][tosw] += flowbytes\n",
    "trafficfile = f\"{homedir}/DRing/src/emp/datacentre/trafficfiles/{trafficname}\"\n",
    "with open(trafficfile,'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        tokens = line.split(',')\n",
    "        interval = int(tokens[0])\n",
    "        fromsvr = int(tokens[1])\n",
    "        tosvr = int(tokens[2])\n",
    "        if fromsvr>=numserver or tosvr>=numserver: continue\n",
    "        fromsw = serverdict[fromsvr]\n",
    "        tosw = serverdict[tosvr]\n",
    "        if fromsw == tosw: continue\n",
    "        flowbytes = int(tokens[3])\n",
    "        traffic[interval][fromsw][tosw] += flowbytes\n",
    "for k in range(numinterval):\n",
    "    for i in range(numsw):\n",
    "        for j in range(numsw):\n",
    "            traffic[k][i][j] /= scaledown\n",
    "\n",
    "totaltraffic_per_interval = list()\n",
    "for k in range(numinterval):\n",
    "    totaltraffic = 0\n",
    "    for i in range(numsw):\n",
    "        for j in range(numsw):\n",
    "            totaltraffic += traffic[k][i][j]\n",
    "    totaltraffic_per_interval.append(totaltraffic)\n",
    "\n",
    "\n",
    "# read in throughput with equal path weights\n",
    "# equalthroughput = dict()\n",
    "# for routingname in routingnamearr:\n",
    "#     equalthroughput[routingname] = dict()\n",
    "# with open(spatialfile,'r') as f:\n",
    "#     lines = f.readlines()\n",
    "#     for line in lines:\n",
    "#         tokens = line.split(',')\n",
    "#         routingname = tokens[0]\n",
    "#         interval = int(tokens[1])\n",
    "#         throughput = float(tokens[2])\n",
    "#         equalthroughput[routingname][interval] = throughput\n",
    "\n",
    "\n",
    "# compute & solve\n",
    "netpathfile = f\"{homedir}/DRing/src/emp/datacentre/netpathfiles/netpath_{routingname}_{graphname}.txt\"\n",
    "for interval in range(3,7):\n",
    "    computeqivarfile = f\"{homedir}/DRing/src/emp/datacentre/qivarfiles/ivar_compute_{graphname}_{numfaillink}_{fseed}_{trafficname}_{interval}_{routingname}_{method}_{crossover}\"\n",
    "    compute(interval,graphfile,numsw,netpathfile,traffic,method,crossover,computeqivarfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantize\n",
    "# copy from makepathweightfiles/makepathweightfiles.ipynb\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "def quantize_to_multiple_of_1_64(x,factor):\n",
    "    return round(x * factor) / factor\n",
    "\n",
    "def quantize_and_adjust_with_indices(indexed_values,factor,isprint=False):\n",
    "    if isprint: print(indexed_values)\n",
    "    # Quantize each value\n",
    "    quantized_values = [(i, quantize_to_multiple_of_1_64(v,factor)) for i, v in indexed_values]\n",
    "    if isprint: print(quantized_values)\n",
    "    \n",
    "    # Calculate the current sum\n",
    "    current_sum = sum(v for _, v in quantized_values)\n",
    "    \n",
    "    # Calculate the difference between the sum and 1\n",
    "    difference = current_sum - 1\n",
    "    if isprint: print(f\"currsum={current_sum}, difference={difference}\")\n",
    "\n",
    "    random_values = [(v, random.random()) for v in quantized_values]\n",
    "    if isprint: print(random_values)\n",
    "    \n",
    "    if difference != 0:\n",
    "        if difference > 0:\n",
    "            # Sort indices by quantized values to adjust the largest values first\n",
    "            sorted_random = sorted(random_values, key=lambda x: (x[0][1],x[1]), reverse=True)\n",
    "            # sorted_quantized = sorted(quantized_values, key=lambda x: x[1], reverse=True)\n",
    "            sorted_quantized = [v for v,_ in sorted_random]\n",
    "            if isprint: print(sorted_random)\n",
    "            if isprint: print(sorted_quantized)\n",
    "            \n",
    "            while difference !=0:\n",
    "                for i, value in sorted_quantized:\n",
    "                    if difference == 0:\n",
    "                        break\n",
    "                    # Calculate adjustment\n",
    "                    # adjustment = min(difference, value % (1 / factor))\n",
    "                    # adjustment = difference\n",
    "                    new_value = value - 1.0/factor\n",
    "                    \n",
    "                    # Update the quantized values\n",
    "                    quantized_values = [(idx, new_value if idx == i else v) for idx, v in quantized_values]\n",
    "                    difference -= 1.0/factor\n",
    "                \n",
    "        else:\n",
    "            # If the difference is negative (i.e., sum is less than 1), we need to increase values\n",
    "            # sorted_quantized = sorted(quantized_values, key=lambda x: x[1])\n",
    "            sorted_random = sorted(random_values, key=lambda x: (x[0][1],x[1]))\n",
    "            # sorted_quantized = sorted(quantized_values, key=lambda x: x[1], reverse=True)\n",
    "            sorted_quantized = [v for v,_ in sorted_random]\n",
    "            \n",
    "            while difference != 0:\n",
    "                for i, value in sorted_quantized:\n",
    "                    if difference == 0:\n",
    "                        break\n",
    "                    # Calculate adjustment\n",
    "                    # adjustment = min(-difference, (1 / factor) - (value % (1 / factor)))\n",
    "                    # adjustment = -difference\n",
    "                    new_value = value + 1.0/factor\n",
    "                    \n",
    "                    # Update the quantized values\n",
    "                    quantized_values = [(idx, new_value if idx == i else v) for idx, v in quantized_values]\n",
    "                    difference += 1.0/factor\n",
    "    \n",
    "    # Return the list sorted by original indices\n",
    "    # return [v for i, v in sorted(quantized_values)]\n",
    "    if isprint: print(quantized_values)\n",
    "    return quantized_values\n",
    "\n",
    "def check(qvarfile, numsw):\n",
    "    # check qvarfile\n",
    "    checkweightarr = list()\n",
    "    for i in range(numsw):\n",
    "        checkweightarr.append(list())\n",
    "        for j in range(numsw):\n",
    "            checkweightarr[i].append(0)\n",
    "    with open(qvarfile,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.split(',')\n",
    "            fromsw = int(tokens[0])\n",
    "            tosw = int(tokens[1])\n",
    "            pid = int(tokens[2])\n",
    "            weight = float(tokens[3])\n",
    "            checkweightarr[fromsw][tosw] += weight\n",
    "\n",
    "    for fromsw in range(numsw):\n",
    "        for tosw in range(numsw):\n",
    "            if checkweightarr[fromsw][tosw]!=0 and checkweightarr[fromsw][tosw]!=1:\n",
    "                print(f\"from {fromsw} to {tosw}: weight={checkweightarr[fromsw][tosw]}\")\n",
    "\n",
    "def quantize(varfile, qvarfile, factor, numsw):\n",
    "    with open(varfile,'r') as f:\n",
    "        lines = f.readlines()[1:]\n",
    "\n",
    "    valuearr = list()\n",
    "    weightarr = list()\n",
    "    qweightarr = list()\n",
    "    for i in range(numsw):\n",
    "        valuearr.append(list())\n",
    "        weightarr.append(list())\n",
    "        qweightarr.append(list())\n",
    "        for j in range(numsw):\n",
    "            valuearr[i].append(list())\n",
    "            weightarr[i].append(list())\n",
    "            qweightarr[i].append(list()) \n",
    "\n",
    "    for line in lines:\n",
    "        tokens = line.split(',')\n",
    "        varname = tokens[0]\n",
    "        vartokens = varname.split('_')\n",
    "        fromsw = int(vartokens[0])\n",
    "        tosw = int(vartokens[1])\n",
    "        pid = int(vartokens[2])\n",
    "        value = float(tokens[1])\n",
    "        valuearr[fromsw][tosw].append([pid,value])\n",
    "        weightarr[fromsw][tosw].append([pid,value])\n",
    "\n",
    "    for fromsw in range(numsw):\n",
    "        for tosw in range(numsw):\n",
    "            if len(valuearr[fromsw][tosw])>0:\n",
    "                valuesum = 0\n",
    "                for _,value in valuearr[fromsw][tosw]:\n",
    "                    valuesum += value\n",
    "                valuetoweight = 1.0/valuesum\n",
    "                for ipv,pv in enumerate(weightarr[fromsw][tosw]):\n",
    "                    weightarr[fromsw][tosw][ipv][1] = pv[1] * valuetoweight\n",
    "\n",
    "    for fromsw in range(numsw):\n",
    "        for tosw in range(numsw):\n",
    "            if len(weightarr[fromsw][tosw])>0:\n",
    "                qweightarr[fromsw][tosw] = quantize_and_adjust_with_indices(weightarr[fromsw][tosw],factor)\n",
    "\n",
    "    with open(qvarfile,'w') as f:\n",
    "        for fromsw in range(numsw):\n",
    "            for tosw in range(numsw):\n",
    "                if len(qweightarr[fromsw][tosw])>0:\n",
    "                    for pid,weight in qweightarr[fromsw][tosw]:\n",
    "                        if weight>0:\n",
    "                            f.write(f\"{fromsw},{tosw},{pid},{weight}\\n\")\n",
    "\n",
    "    check(qvarfile, numsw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficnamearr = [\"unv1\",\"prv1\"]\n",
    "intervalstart = 4\n",
    "intervalend = 8\n",
    "factor = 64\n",
    "numsw = 80\n",
    "racke = 0\n",
    "H = 1\n",
    "\n",
    "for trafficname in trafficnamearr:\n",
    "    for interval in range(intervalstart, intervalend):\n",
    "        ivarfile = f\"/home/annzhou/DRing/src/emp/datacentre/qivarfiles/ivar_compute_dring_0_0_{trafficname}_{interval-H}_racke{racke}_2_0\"\n",
    "        qivarfile = f\"/home/annzhou/DRing/src/emp/datacentre/qivarfiles/qivar_dring_0_0_{trafficname}_{interval}_racke{racke}_2_0\"\n",
    "        quantize(ivarfile,qivarfile,factor,numsw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate conffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficname = \"unv1\"\n",
    "racke=0\n",
    "multstrarr1 = [\"0 2 25\",\"0 17 100\",\"0 1 4\",\"0 17 50\",\"0 21 50\",\"0 51 100\",\"0 59 100\",\"0 34 50\",\"0 38 50\",\"0 17 20\"]\n",
    "multnamearr1 = range(5,51,5)\n",
    "sseedarr = range(1,6)\n",
    "\n",
    "conffile = f\"/home/annzhou/DRing/src/emp/datacentre/experiments/routing_racke/{trafficname}_{racke}.conf\"\n",
    "make_leafspine = \"MAKE\"\n",
    "make_dring = \"MAKE\"\n",
    "with open(conffile,'w') as f:\n",
    "    for imultstr,multstr in enumerate(multstrarr1):\n",
    "        for sseed in sseedarr:\n",
    "            multname = multnamearr1[imultstr]\n",
    "            # f.write(f\"./run.sh LEAFSPINE {make_leafspine} 80 3072 64 leafspine_{trafficname} NEW_WISC null {multstr} 4 8 0 0 0 0 ecmp 0 {trafficname} {sseed} netpathfiles/netpath_ecmp_leafspine.txt qvarfiles/qvar_leafspine_0_0_ecmp_64 64 50 150 200 0 0 > m_leafspine_{trafficname}_{multname}_{sseed}.log\\n\")\n",
    "            make_leafspine = \"NOMAKE\"\n",
    "            f.write(f\"./run.sh RRG {make_dring} 80 2988 64 dring_{trafficname}_{racke} NEW_WISC graphfiles/ring_supergraph/double_ring/instance1_80_64.edgelist {multstr} 4 8 0 0 0 0 racke {racke} {trafficname} serverfiles/dring_2988_80_64 {sseed} null null 64 50 150 200 0 0 > m_dring_{trafficname}_{racke}_{multname}_{sseed}.log\\n\")\n",
    "            make_dring = \"NOMAKE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficname = \"prv1\"\n",
    "multstrarr1 = [\"0 2 100\",\"0 5 100\",\"0 7 100\",\"0 10 100\",\"0 12 100\",\"0 15 100\",\"0 17 100\",\"0 20 100\",\"0 22 100\",\"0 25 100\",\"0 37 100\",\"0 49 100\",\"0 62 100\",\"0 74 100\"]\n",
    "multnamearr1 = list(range(2,19,2))+list(range(20,61,10))\n",
    "sseedarr = range(1,6)\n",
    "\n",
    "conffile = f\"/home/annzhou/DRing/src/emp/datacentre/experiments/routing_racke/{trafficname}_{racke}.conf\"\n",
    "make_leafspine = \"MAKE\"\n",
    "make_dring = \"MAKE\"\n",
    "with open(conffile,'w') as f:\n",
    "    for imultstr,multstr in enumerate(multstrarr1):\n",
    "        for sseed in sseedarr:\n",
    "            multname = multnamearr1[imultstr]\n",
    "            # f.write(f\"./run.sh LEAFSPINE {make_leafspine} 80 3072 64 leafspine_{trafficname} NEW_WISC null {multstr} 4 8 0 0 0 0 ecmp 0 {trafficname} {sseed} netpathfiles/netpath_ecmp_leafspine.txt qvarfiles/qvar_leafspine_0_0_ecmp_64 64 50 150 200 0 0 > m_leafspine_{trafficname}_{multname}_{sseed}.log\\n\")\n",
    "            make_leafspine = \"NOMAKE\"\n",
    "            f.write(f\"./run.sh RRG {make_dring} 80 2988 64 dring_{trafficname}_{racke} NEW_WISC graphfiles/ring_supergraph/double_ring/instance1_80_64.edgelist {multstr} 4 8 0 0 0 0 racke {racke} {trafficname} serverfiles/dring_2988_80_64 {sseed} null null 64 50 150 200 0 0 > m_dring_{trafficname}_{racke}_{multname}_{sseed}.log\\n\")\n",
    "            make_dring = \"NOMAKE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficname = \"cluster_a\"\n",
    "racke=0\n",
    "multstrarr1 = [\"4 8 100\",\"8 17 100\",\"12 25 100\",\"16 34 100\",\"20 42 100\",\"24 51 100\",\"28 59 100\",\"32 67 100\",\"36 76 100\"]\n",
    "multnamearr1 = range(10,91,10)\n",
    "sseedarr = range(1,6)\n",
    "\n",
    "conffile = f\"/home/annzhou/DRing/src/emp/datacentre/experiments/routing_racke/{trafficname}_{racke}.conf\"\n",
    "make_leafspine = \"MAKE\"\n",
    "make_dring = \"MAKE\"\n",
    "with open(conffile,'w') as f:\n",
    "    for imultstr,multstr in enumerate(multstrarr1):\n",
    "        for sseed in sseedarr:\n",
    "            multname = multnamearr1[imultstr]\n",
    "            # f.write(f\"./run.sh LEAFSPINE {make_leafspine} 80 3072 64 leafspine_{trafficname} NEW_WISC null {multstr} 24 48 0 0 0 0 ecmp 0 {trafficname} {sseed} netpathfiles/netpath_ecmp_leafspine.txt qvarfiles/qvar_leafspine_0_0_ecmp_64 64 50 150 200 0 0 > m_leafspine_{trafficname}_{multname}_{sseed}.log\\n\")\n",
    "            make_leafspine = \"NOMAKE\"\n",
    "            f.write(f\"./run.sh RRG {make_dring} 80 2988 64 dring_{trafficname}_{racke} NEW_WISC graphfiles/ring_supergraph/double_ring/instance1_80_64.edgelist {multstr} 24 48 0 0 0 0 racke {racke} {trafficname} serverfiles/dring_2988_80_64 {sseed} null null 64 50 150 200 0 0 > m_dring_{trafficname}_{racke}_{multname}_{sseed}.log\\n\")\n",
    "            make_dring = \"NOMAKE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficname = \"cluster_b\"\n",
    "racke=0\n",
    "multstrarr1 = [\"0 27 100\",\"0 53 100\",\"0 80 100\",\"1 7 100\",\"1 34 100\",\"1 60 100\",\"1 87 100\",\"2 14 100\",\"2 40 100\"]\n",
    "multnamearr1 = range(10,91,10)\n",
    "sseedarr = range(1,6)\n",
    "\n",
    "conffile = f\"/u/az6922/DRing/src/emp/datacentre/experiments/routing_racke/{trafficname}_{racke}.conf\"\n",
    "make_leafspine = \"MAKE\"\n",
    "make_dring = \"MAKE\"\n",
    "with open(conffile,'w') as f:\n",
    "    for imultstr,multstr in enumerate(multstrarr1):\n",
    "        for sseed in sseedarr:\n",
    "            multname = multnamearr1[imultstr]\n",
    "            # f.write(f\"./run.sh LEAFSPINE {make_leafspine} 80 3072 64 leafspine_{trafficname} NEW_WISC null {multstr} 24 48 0 0 0 0 ecmp 0 {trafficname} {sseed} netpathfiles/netpath_ecmp_leafspine.txt qvarfiles/qvar_leafspine_0_0_ecmp_64 64 50 150 200 0 0 > m_leafspine_{trafficname}_{multname}_{sseed}.log\\n\")\n",
    "            make_leafspine = \"NOMAKE\"\n",
    "            f.write(f\"./run.sh RRG {make_dring} 80 2988 64 dring_{trafficname}_{racke} NEW_WISC graphfiles/ring_supergraph/double_ring/instance1_80_64.edgelist {multstr} 24 48 0 0 0 0 racke {racke} {trafficname} serverfiles/dring_2988_80_64 {sseed} null null 64 50 150 200 0 0 > m_dring_{trafficname}_{racke}_{multname}_{sseed}.log\\n\")\n",
    "            make_dring = \"NOMAKE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficname = \"cluster_c\"\n",
    "racke=0\n",
    "multstrarr1 = [\"0 19 100\",\"0 38 100\",\"0 57 100\",\"0 75 100\",\"0 94 100\",\"1 13 100\",\"1 32 100\",\"1 51 100\",\"1 70 100\"]\n",
    "multnamearr1 = range(10,91,10)\n",
    "sseedarr = range(1,6)\n",
    "\n",
    "conffile = f\"/home/annzhou/DRing/src/emp/datacentre/experiments/routing_racke/{trafficname}_{racke}.conf\"\n",
    "make_leafspine = \"MAKE\"\n",
    "make_dring = \"MAKE\"\n",
    "with open(conffile,'w') as f:\n",
    "    for imultstr,multstr in enumerate(multstrarr1):\n",
    "        for sseed in sseedarr:\n",
    "            multname = multnamearr1[imultstr]\n",
    "            # f.write(f\"./run.sh LEAFSPINE {make_leafspine} 80 3072 64 leafspine_{trafficname} NEW_WISC null {multstr} 24 48 0 0 0 0 ecmp 0 {trafficname} {sseed} netpathfiles/netpath_ecmp_leafspine.txt qvarfiles/qvar_leafspine_0_0_ecmp_64 64 50 150 200 0 0 > m_leafspine_{trafficname}_{multname}_{sseed}.log\\n\")\n",
    "            make_leafspine = \"NOMAKE\"\n",
    "            f.write(f\"./run.sh RRG {make_dring} 80 2988 64 dring_{trafficname}_{racke} NEW_WISC graphfiles/ring_supergraph/double_ring/instance1_80_64.edgelist {multstr} 24 48 0 0 0 0 racke {racke} {trafficname} serverfiles/dring_2988_80_64 {sseed} null null 64 50 150 200 0 0 > m_dring_{trafficname}_{racke}_{multname}_{sseed}.log\\n\")\n",
    "            make_dring = \"NOMAKE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate sumfile & picklefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "trafficname = \"prv1\"\n",
    "\n",
    "homedir = \"/home/annzhou\"\n",
    "datadict = dict()\n",
    "conffile = f\"{homedir}/DRing/src/emp/datacentre/experiments/routing_fhi/{trafficname}.conf\"\n",
    "sumfile = f\"{homedir}/DRing/src/emp/datacentre/experiments/routing_fhi/{trafficname}.txt\"\n",
    "with open(conffile,'r') as f:\n",
    "    with open(sumfile,'a') as sumf:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.split()\n",
    "            logfile = tokens[32]\n",
    "            sizelist = list()\n",
    "            durationlist = list()\n",
    "            startlist = list()\n",
    "            with open(f\"{homedir}/DRing/src/emp/datacentre/{logfile}\",'r') as logf:\n",
    "                print(logfile)\n",
    "                loglines = logf.readlines()\n",
    "                for logline in loglines:\n",
    "                    logtokens = logline.split()\n",
    "                    if logtokens[0] == \"FCT\":\n",
    "                        durationlist.append(float(logtokens[2]))\n",
    "                        sizelist.append(int(logtokens[1]))\n",
    "                        startlist.append(float(logtokens[3]))\n",
    "            datadict[logfile] = [durationlist,sizelist,startlist]\n",
    "            durationlist.sort()\n",
    "            sumf.write(f\"{logfile}\\t{sum(durationlist)/len(durationlist)}\\t{durationlist[int(len(durationlist)*0.5)]}\\t{durationlist[int(len(durationlist)*0.99)]}\\t{durationlist[int(len(durationlist)*0.9999)]}\\n\")\n",
    "\n",
    "with open(f'{homedir}/DRing/src/emp/datacentre/experiments/routing_fhi/{trafficname}.pickle', 'wb') as handle:\n",
    "    pickle.dump(datadict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete log,run files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "homedir = \"/home/annzhou\"\n",
    "dringdir = f\"{homedir}/DRing/src/emp/datacentre\"\n",
    "for filedir in os.listdir(dringdir):\n",
    "    if os.path.isfile(os.path.join(dringdir, filedir)):\n",
    "        tokens = filedir.split(\"_\")\n",
    "        if tokens[0]==\"m\" and (tokens[1]==\"leafspine\" or tokens[1]==\"dring\") and tokens[-1].split('.')[-1]==\"log\":\n",
    "            print(filedir)\n",
    "            os.remove(f\"{homedir}/DRing/src/emp/datacentre/{filedir}\")\n",
    "        if (tokens[0]==\"rrg\" or tokens[0]==\"leafspine\") and tokens[1]==\"log\":\n",
    "            print(filedir)\n",
    "            os.remove(f\"{homedir}/DRing/src/emp/datacentre/{filedir}\")\n",
    "        if (tokens[-1]==\"run\") and ((tokens[0]==\"leafspine\" and tokens[1]==\"leafspine\") or (tokens[0]==\"rrg\" and (tokens[1]==\"rrg\" or tokens[1]==\"dring\"))):\n",
    "            print(filedir)\n",
    "            os.remove(f\"{homedir}/DRing/src/emp/datacentre/{filedir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
